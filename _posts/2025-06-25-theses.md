---
layout: static-post
title: MSc and BSc theses
---

### Shorter secret keys in multivariate cryptography through optimal subspace representations

*Post-quantum (quadratic multivariate) cryptography.*

Performed at Universitat Politècnica de Catalunya, as part of the Master's degree in Advanced Mathematics and Mathematical Engineering, under the advisorship of Javier Herranz, obtaining Honors.

> *Multivariate cryptography* (MVQC) is currently one of the most promising families of seemingly quantum-safe cryptographical schemes. However, it often suffers from extremely large key sizes or ad-hoc security assumptions. In this thesis we deal with a MVQC scheme, *Unbalanced Oil and Vinegar* (UOV), leveraging a reformulation of it recently given by Beullens ([6]) with the aim of exploring how much one can reduce the size of secret keys while retaining practicality. Moreover, we focus on two simplifications commonly applied to UOV — these are justified simply by seeing that one can apply the same hardness assumptions that buttress standard UOV. We show that (with some concessions), it can be proven that they retain security just from the assumption that standard UOV is secure.

Full thesis text available [here]({{ '/' | relative_url }}files/Memoria_TFM_ArnalXavi.pdf).

### Analysis of metric and topological estimators of generalization in deep learning

*Deep Learning, Topological Data Analysis.*

Performed at Universitat de Barcelona, as part of the Bachelor's degree in Mathematics, under the advisorship of Carles Casacuberta, Ciprian Corneanu, Sergio Escalera, and Meysam Madadi, obtaining a mark of 94%.

> One of the most important notions in deep learning (or in statistical learning) is that of *generalization* &mdash; the capacity of a model trained on some data to perform well on the latent source of truth from which its training data has been drawn. Though some ways exist to estimate a model's capacity to generalize from its internal structure or performance during training, it is generally not well understood what distinguishes the models that generalize from the ones that do not. In this text, we formalize, expand on and give some basic results regarding a previously proposed way to extract a finite metric space from a neural network. This is done with the intention of using this finite metric space as a starting point to perform techniques from topological data analysis, and use the data obtained about the latent topology of this metric space to predict the capacity to generalize of a model. Moreover, we compare the performance of several estimators with other known predictors of generalization, and with estimators based solely on the metric information obtained from the model.

Full thesis text available [here]({{ '/' | relative_url }}files/Memoria_TFG_ArnalXavi.pdf).

